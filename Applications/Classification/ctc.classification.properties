dir_repositpry = /gpfs0/santagoGrp/data/WFU_prior_CTC_data
#dir_repositpry = /home/haxu/workspace/projects/CTC_Classification/Data/letter-recognition.arff
dir_output = /home/haxu/workspace/projects/CTC_Classification/Output/
model_file_prefix = model_
predict_file = predict.csv
logfile_train = train.log
logfile_test = test.log

# if this flag is true, the train/test file will be generated using dir_repository, dir_output, FP_TP_dist, train_percent. In this case, the file_train and file_test is NOT used.
# if this flag is false, the file_train and file_test below will be used.
generate_train_test_file = true

file_train = /home/haxu/workspace/projects/CTC_Classification/Data/weka/ExtractedPolyps.mat.matched.2.csv.arff.1.0.train80.0.arff
file_test = /home/haxu/workspace/projects/CTC_Classification/Data/weka/ExtractedPolyps.mat.matched.2.csv.arff.1.0.test20.0.arff

# 'OVERSAMPLE', 'SUBSAMPLE', or 'SMOTE'
resample_scheme = SMOTE

# FT and TP class distribution: (assume TP samples are always less than FP)
#   0 -- natural class distribution
#   1 -- uniform class distribution (the number of FP and TP samples are same)
#   float number -- ratio of FP/TP for SUBSAMPLE, ratio of TP/FP for OVERSAMPLE
#                   and SMOTE
dist = 5.0
train_percent = 80.0
use_all_as_test_data = false

remove_feature = 1,2,3,4

classifiers = \
weka.classifiers.functions.SMO;\
weka.classifiers.functions.MultilayerPerceptron;\
weka.classifiers.functions.RBFNetwork;\
weka.classifiers.functions.SimpleLogistic;\
weka.classifiers.meta.AdaBoostM1;\
weka.classifiers.meta.Bagging;\
weka.classifiers.meta.LogitBoost

