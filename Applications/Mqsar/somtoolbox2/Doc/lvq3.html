<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html><head><title>SOM Toolbox / lvq3 </title></head>
<body bgcolor=#f0f0f0>
<table border=0 width="100%" cellpadding=0 cellspacing=0><tr>
<td valign=baseline><font size=+2>SOM Toolbox</font></td>
<td valign=baseline align=center><a href="somtoolbox.html">Online documentation</td>
<td valign=baseline align=right><a href="http://www.cis.hut.fi/projects/somtoolbox/" target="_top">http://www.cis.hut.fi/projects/somtoolbox/</a></td>
</tr></table>
<hr>
<H1> lvq3 </H1>

<H3> Purpose </H3>

<PRE>
 Trains codebook with the LVQ3 -algorithm (described below).

</PRE>

<H3> Syntax </H3>

<UL><PRE>
 sM = lvq3(sM, data, rlen, alpha, win, epsilon)

</PRE></UL>

<H3> Description </H3>

<PRE>
 Trains codebook with the LVQ3 -algorithm. Codebook contains a number
 of vectors (mi, i=1,2,...,n) and so does data (vectors xj, j=1,2,...k).
 Both vector sets are classified: vectors may have a class (classes are
 set to data- or map -structure's 'labels' -field. For each xj the two 
 closest codebookvectors mc1 and mc2 are searched (euclidean distances
 d1 and d2). xj must fall into the zone of window. That happens if:

    min(d1/d2, d2/d1) > s, where s = (1-win) / (1+win).

 If xj belongs to the same class of one of the mc1 and mc1, codebook
 is updated as follows (let mc1 belong to the same class as xj):
    mc1(t+1) = mc1(t) + alpha * (xj(t) - mc1(t))
    mc2(t+1) = mc2(t) - alpha * (xj(t) - mc2(t))
 If both mc1 and mc2 belong to the same class as xj, codebook is
 updated as follows:
    mc1(t+1) = mc1(t) + epsilon * alpha * (xj(t) - mc1(t))
    mc2(t+1) = mc2(t) + epsilon * alpha * (xj(t) - mc2(t))
 Otherwise updating is not performed.

 Argument 'rlen' tells how many times training -sequence is performed.

 Argument 'alpha' is recommended to be smaller than 0.1 and argument
 'epsilon' should be between 0.1 and 0.5.

 NOTE: does not take mask into account.

</PRE>

<H3> References </H3>

<PRE>
 Kohonen, T., "Self-Organizing Map", 2nd ed., Springer-Verlag, 
    Berlin, 1995, pp. 181-182.

 See also LVQ_PAK from http://www.cis.hut.fi/research/som_lvq_pak.shtml

</PRE>

<H3> Required input arguments </H3>

<PRE>
  sM                The data to be trained.
          (struct)  A map struct.

  data              The data to use in training.
          (struct)  A data struct.

  rlen    (integer) Running length of LVQ3 -algorithm.

  alpha   (float)   Learning rate used in training, e.g. 0.05

  win     (float)   Window length, e.g. 0.25

  epsilon (float)   Relative learning parameter, e.g. 0.3

</PRE>

<H3> Output arguments </H3>

<PRE>
  sM          Trained data.
          (struct)  A map struct.

</PRE>

<H3> Example </H3>

<PRE>
   lab = unique(sD.labels(:,1));         % different classes
   mu = length(lab)*5;                   % 5 prototypes for each    
   sM = som_randinit(sD,'msize',[mu 1]); % initial prototypes
   sM.labels = [lab;lab;lab;lab;lab];    % their classes
   sM = lvq1(sM,sD,50*mu,0.05);          % use LVQ1 to adjust
                                         % the prototypes      
   sM = lvq3(sM,sD,50*mu,0.05,0.2,0.3);  % then use LVQ3 

</PRE>

<H3> See also </H3>

<TABLE NOBORDER WIDTH=80%>
<TR><TD><a href="lvq1.html"><B>lvq1</B></a>
<TD> Use LVQ1 algorithm for training.
<TR><TD><a href="som_supervised.html"><B>som_supervised</B></a>
<TD> Train SOM using supervised training.
<TR><TD><a href="som_seqtrain.html"><B>som_seqtrain</B></a>
<TD> Train SOM with sequential algorithm.
</TABLE>
<p>
<hr><br>
<br>
<!-- Last updated: May 30 2002 -->
</body>
</html>
